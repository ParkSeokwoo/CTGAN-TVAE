{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "\n",
        "class DataPrep(object):\n",
        "\n",
        "    def __init__(self, raw_df: pd.DataFrame, categorical: list, log:list, mixed:dict, general:list, non_categorical:list, integer:list, type:dict, test_ratio:float):\n",
        "\n",
        "\n",
        "        self.categorical_columns = categorical\n",
        "        self.log_columns = log\n",
        "        self.mixed_columns = mixed\n",
        "        self.general_columns = general\n",
        "        self.non_categorical_columns = non_categorical\n",
        "        self.integer_columns = integer\n",
        "        self.column_types = dict()\n",
        "        self.column_types[\"categorical\"] = []\n",
        "        self.column_types[\"mixed\"] = {}\n",
        "        self.column_types[\"general\"] = []\n",
        "        self.column_types[\"non_categorical\"] = []\n",
        "        self.lower_bounds = {}\n",
        "        self.label_encoder_list = []\n",
        "\n",
        "        problem = list(type.keys())[0]\n",
        "        target_col = list(type.values())[0]\n",
        "        if problem:\n",
        "            y_real = raw_df[target_col]\n",
        "            X_real = raw_df.drop(columns=[target_col])\n",
        "\n",
        "            if problem==\"Classification\":\n",
        "                X_train_real, _, y_train_real, _ = model_selection.train_test_split(X_real ,y_real, test_size=test_ratio, stratify=y_real,random_state=42)\n",
        "            else:\n",
        "                X_train_real, _, y_train_real, _ = model_selection.train_test_split(X_real ,y_real, test_size=test_ratio,random_state=42)\n",
        "\n",
        "            X_train_real[target_col]= y_train_real\n",
        "\n",
        "            self.df = X_train_real\n",
        "\n",
        "        else:\n",
        "            self.df = raw_df\n",
        "        self.df = self.df.replace(r' ', np.nan)\n",
        "        self.df = self.df.fillna('empty')\n",
        "\n",
        "        all_columns= set(self.df.columns)\n",
        "        irrelevant_missing_columns = set(self.categorical_columns)\n",
        "        relevant_missing_columns = list(all_columns - irrelevant_missing_columns)\n",
        "\n",
        "        for i in relevant_missing_columns:\n",
        "            if i in self.log_columns:\n",
        "                if \"empty\" in list(self.df[i].values):\n",
        "                    self.df[i] = self.df[i].apply(lambda x: -9999999 if x==\"empty\" else x)\n",
        "                    self.mixed_columns[i] = [-9999999]\n",
        "            elif i in list(self.mixed_columns.keys()):\n",
        "                if \"empty\" in list(self.df[i].values):\n",
        "                    self.df[i] = self.df[i].apply(lambda x: -9999999 if x==\"empty\" else x )\n",
        "                    self.mixed_columns[i].append(-9999999)\n",
        "            else:\n",
        "                if \"empty\" in list(self.df[i].values):\n",
        "                    self.df[i] = self.df[i].apply(lambda x: -9999999 if x==\"empty\" else x)\n",
        "                    self.mixed_columns[i] = [-9999999]\n",
        "\n",
        "        if self.log_columns:\n",
        "            for log_column in self.log_columns:\n",
        "                valid_indices = []\n",
        "                for idx,val in enumerate(self.df[log_column].values):\n",
        "                    if val!=-9999999:\n",
        "                        valid_indices.append(idx)\n",
        "                eps = 1\n",
        "                lower = np.min(self.df[log_column].iloc[valid_indices].values)\n",
        "                self.lower_bounds[log_column] = lower\n",
        "                if lower>0:\n",
        "                    self.df[log_column] = self.df[log_column].apply(lambda x: np.log(x) if x!=-9999999 else -9999999)\n",
        "                elif lower == 0:\n",
        "                    self.df[log_column] = self.df[log_column].apply(lambda x: np.log(x+eps) if x!=-9999999 else -9999999)\n",
        "                else:\n",
        "                    self.df[log_column] = self.df[log_column].apply(lambda x: np.log(x-lower+eps) if x!=-9999999 else -9999999)\n",
        "\n",
        "        for column_index, column in enumerate(self.df.columns):\n",
        "            if column in self.categorical_columns:\n",
        "                label_encoder = preprocessing.LabelEncoder()\n",
        "                self.df[column] = self.df[column].astype(str)\n",
        "                label_encoder.fit(self.df[column])\n",
        "                current_label_encoder = dict()\n",
        "                current_label_encoder['column'] = column\n",
        "                current_label_encoder['label_encoder'] = label_encoder\n",
        "                transformed_column = label_encoder.transform(self.df[column])\n",
        "                self.df[column] = transformed_column\n",
        "                self.label_encoder_list.append(current_label_encoder)\n",
        "                self.column_types[\"categorical\"].append(column_index)\n",
        "\n",
        "                if column in self.general_columns:\n",
        "                    self.column_types[\"general\"].append(column_index)\n",
        "\n",
        "                if column in self.non_categorical_columns:\n",
        "                    self.column_types[\"non_categorical\"].append(column_index)\n",
        "\n",
        "            elif column in self.mixed_columns:\n",
        "                self.column_types[\"mixed\"][column_index] = self.mixed_columns[column]\n",
        "\n",
        "            elif column in self.general_columns:\n",
        "                self.column_types[\"general\"].append(column_index)\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "    def inverse_prep(self, data, eps=1):\n",
        "\n",
        "        df_sample = pd.DataFrame(data,columns=self.df.columns)\n",
        "\n",
        "        for i in range(len(self.label_encoder_list)):\n",
        "            le = self.label_encoder_list[i][\"label_encoder\"]\n",
        "            df_sample[self.label_encoder_list[i][\"column\"]] = df_sample[self.label_encoder_list[i][\"column\"]].astype(int)\n",
        "            df_sample[self.label_encoder_list[i][\"column\"]] = le.inverse_transform(df_sample[self.label_encoder_list[i][\"column\"]])\n",
        "\n",
        "        if self.log_columns:\n",
        "            for i in df_sample:\n",
        "                if i in self.log_columns:\n",
        "                    lower_bound = self.lower_bounds[i]\n",
        "                    if lower_bound>0:\n",
        "                        df_sample[i].apply(lambda x: np.exp(x))\n",
        "                    elif lower_bound==0:\n",
        "                        df_sample[i] = df_sample[i].apply(lambda x: np.ceil(np.exp(x)-eps) if (np.exp(x)-eps) < 0 else (np.exp(x)-eps))\n",
        "                    else:\n",
        "                        df_sample[i] = df_sample[i].apply(lambda x: np.exp(x)-eps+lower_bound)\n",
        "\n",
        "        if self.integer_columns:\n",
        "            for column in self.integer_columns:\n",
        "                df_sample[column]= (np.round(df_sample[column].values))\n",
        "                df_sample[column] = df_sample[column].astype(int)\n",
        "\n",
        "        df_sample.replace(-9999999, np.nan,inplace=True)\n",
        "        df_sample.replace('empty', np.nan,inplace=True)\n",
        "\n",
        "        return df_sample"
      ],
      "metadata": {
        "id": "ee_XfHlGjHPY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "\n",
        "class DataTransformer():\n",
        "\n",
        "    def __init__(self, train_data=pd.DataFrame, categorical_list=[], mixed_dict={}, general_list=[], non_categorical_list=[], n_clusters=10, eps=0.005):\n",
        "        self.meta = None\n",
        "        self.n_clusters = n_clusters\n",
        "        self.eps = eps\n",
        "        self.train_data = train_data\n",
        "        self.categorical_columns= categorical_list\n",
        "        self.mixed_columns= mixed_dict\n",
        "        self.general_columns = general_list\n",
        "        self.non_categorical_columns= non_categorical_list\n",
        "\n",
        "    def get_metadata(self):\n",
        "\n",
        "        meta = []\n",
        "\n",
        "        for index in range(self.train_data.shape[1]):\n",
        "            column = self.train_data.iloc[:,index]\n",
        "            if index in self.categorical_columns:\n",
        "                if index in self.non_categorical_columns:\n",
        "                    meta.append({\n",
        "                      \"name\": index,\n",
        "                      \"type\": \"continuous\",\n",
        "                      \"min\": column.min(),\n",
        "                      \"max\": column.max(),\n",
        "                    })\n",
        "                else:\n",
        "                    mapper = column.value_counts().index.tolist()\n",
        "                    meta.append({\n",
        "                        \"name\": index,\n",
        "                        \"type\": \"categorical\",\n",
        "                        \"size\": len(mapper),\n",
        "                        \"i2s\": mapper\n",
        "                    })\n",
        "\n",
        "            elif index in self.mixed_columns.keys():\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": \"mixed\",\n",
        "                    \"min\": column.min(),\n",
        "                    \"max\": column.max(),\n",
        "                    \"modal\": self.mixed_columns[index]\n",
        "                })\n",
        "            else:\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": \"continuous\",\n",
        "                    \"min\": column.min(),\n",
        "                    \"max\": column.max(),\n",
        "                })\n",
        "\n",
        "        return meta\n",
        "\n",
        "    def fit(self):\n",
        "        data = self.train_data.values\n",
        "        self.meta = self.get_metadata()\n",
        "        model = []\n",
        "        self.ordering = []\n",
        "        self.output_info = []\n",
        "        self.output_dim = 0\n",
        "        self.components = []\n",
        "        self.filter_arr = []\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == \"continuous\":\n",
        "                if id_ not in self.general_columns:\n",
        "                  gm = BayesianGaussianMixture(\n",
        "                      n_components = self.n_clusters,\n",
        "                      weight_concentration_prior_type='dirichlet_process',\n",
        "                      weight_concentration_prior=0.001,\n",
        "                      max_iter=100,n_init=1, random_state=42)\n",
        "                  gm.fit(data[:, id_].reshape([-1, 1]))\n",
        "                  mode_freq = (pd.Series(gm.predict(data[:, id_].reshape([-1, 1]))).value_counts().keys())\n",
        "                  model.append(gm)\n",
        "                  old_comp = gm.weights_ > self.eps\n",
        "                  comp = []\n",
        "                  for i in range(self.n_clusters):\n",
        "                      if (i in (mode_freq)) & old_comp[i]:\n",
        "                          comp.append(True)\n",
        "                      else:\n",
        "                          comp.append(False)\n",
        "                  self.components.append(comp)\n",
        "                  self.output_info += [(1, 'tanh','no_g'), (np.sum(comp), 'softmax')]\n",
        "                  self.output_dim += 1 + np.sum(comp)\n",
        "                else:\n",
        "                  model.append(None)\n",
        "                  self.components.append(None)\n",
        "                  self.output_info += [(1, 'tanh','yes_g')]\n",
        "                  self.output_dim += 1\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                gm1 = BayesianGaussianMixture(\n",
        "                    n_components = self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, max_iter=100,\n",
        "                    n_init=1,random_state=42)\n",
        "                gm2 = BayesianGaussianMixture(\n",
        "                    n_components = self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001, max_iter=100,\n",
        "                    n_init=1,random_state=42)\n",
        "\n",
        "                gm1.fit(data[:, id_].reshape([-1, 1]))\n",
        "\n",
        "                filter_arr = []\n",
        "                for element in data[:, id_]:\n",
        "                    if element not in info['modal']:\n",
        "                        filter_arr.append(True)\n",
        "                    else:\n",
        "                        filter_arr.append(False)\n",
        "\n",
        "                gm2.fit(data[:, id_][filter_arr].reshape([-1, 1]))\n",
        "                mode_freq = (pd.Series(gm2.predict(data[:, id_][filter_arr].reshape([-1, 1]))).value_counts().keys())\n",
        "                self.filter_arr.append(filter_arr)\n",
        "                model.append((gm1,gm2))\n",
        "\n",
        "                old_comp = gm2.weights_ > self.eps\n",
        "\n",
        "                comp = []\n",
        "\n",
        "                for i in range(self.n_clusters):\n",
        "                    if (i in (mode_freq)) & old_comp[i]:\n",
        "                        comp.append(True)\n",
        "                    else:\n",
        "                        comp.append(False)\n",
        "\n",
        "                self.components.append(comp)\n",
        "\n",
        "                self.output_info += [(1, 'tanh',\"no_g\"), (np.sum(comp) + len(info['modal']), 'softmax')]\n",
        "                self.output_dim += 1 + np.sum(comp) + len(info['modal'])\n",
        "            else:\n",
        "                model.append(None)\n",
        "                self.components.append(None)\n",
        "                self.output_info += [(info['size'], 'softmax')]\n",
        "                self.output_dim += info['size']\n",
        "        self.model = model\n",
        "\n",
        "    def transform(self, data, ispositive = False, positive_list = None):\n",
        "        values = []\n",
        "        mixed_counter = 0\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            current = data[:, id_]\n",
        "            if info['type'] == \"continuous\":\n",
        "                if id_ not in self.general_columns:\n",
        "                  current = current.reshape([-1, 1])\n",
        "                  means = self.model[id_].means_.reshape((1, self.n_clusters))\n",
        "                  stds = np.sqrt(self.model[id_].covariances_).reshape((1, self.n_clusters))\n",
        "                  features = np.empty(shape=(len(current),self.n_clusters))\n",
        "                  if ispositive == True:\n",
        "                      if id_ in positive_list:\n",
        "                          features = np.abs(current - means) / (4 * stds)\n",
        "                  else:\n",
        "                      features = (current - means) / (4 * stds)\n",
        "\n",
        "                  probs = self.model[id_].predict_proba(current.reshape([-1, 1]))\n",
        "                  n_opts = sum(self.components[id_])\n",
        "                  features = features[:, self.components[id_]]\n",
        "                  probs = probs[:, self.components[id_]]\n",
        "\n",
        "                  opt_sel = np.zeros(len(data), dtype='int')\n",
        "                  for i in range(len(data)):\n",
        "                      pp = probs[i] + 1e-6\n",
        "                      pp = pp / sum(pp)\n",
        "                      opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
        "\n",
        "                  idx = np.arange((len(features)))\n",
        "                  features = features[idx, opt_sel].reshape([-1, 1])\n",
        "                  features = np.clip(features, -.99, .99)\n",
        "                  probs_onehot = np.zeros_like(probs)\n",
        "                  probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
        "\n",
        "                  re_ordered_phot = np.zeros_like(probs_onehot)\n",
        "\n",
        "                  col_sums = probs_onehot.sum(axis=0)\n",
        "\n",
        "\n",
        "                  n = probs_onehot.shape[1]\n",
        "                  largest_indices = np.argsort(-1*col_sums)[:n]\n",
        "                  self.ordering.append(largest_indices)\n",
        "                  for id,val in enumerate(largest_indices):\n",
        "                      re_ordered_phot[:,id] = probs_onehot[:,val]\n",
        "\n",
        "\n",
        "                  values += [features, re_ordered_phot]\n",
        "\n",
        "                else:\n",
        "\n",
        "                  self.ordering.append(None)\n",
        "\n",
        "                  if id_ in self.non_categorical_columns:\n",
        "                    info['min'] = -1e-3\n",
        "                    info['max'] = info['max'] + 1e-3\n",
        "\n",
        "                  current = (current - (info['min'])) / (info['max'] - info['min'])\n",
        "                  current = current * 2 - 1\n",
        "                  current = current.reshape([-1, 1])\n",
        "                  values.append(current)\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                means_0 = self.model[id_][0].means_.reshape([-1])\n",
        "                stds_0 = np.sqrt(self.model[id_][0].covariances_).reshape([-1])\n",
        "\n",
        "                zero_std_list = []\n",
        "                means_needed = []\n",
        "                stds_needed = []\n",
        "\n",
        "                for mode in info['modal']:\n",
        "                    if mode!=-9999999:\n",
        "                        dist = []\n",
        "                        for idx,val in enumerate(list(means_0.flatten())):\n",
        "                            dist.append(abs(mode-val))\n",
        "                        index_min = np.argmin(np.array(dist))\n",
        "                        zero_std_list.append(index_min)\n",
        "                    else: continue\n",
        "\n",
        "                for idx in zero_std_list:\n",
        "                    means_needed.append(means_0[idx])\n",
        "                    stds_needed.append(stds_0[idx])\n",
        "\n",
        "\n",
        "                mode_vals = []\n",
        "\n",
        "                for i,j,k in zip(info['modal'],means_needed,stds_needed):\n",
        "                    this_val  = np.abs(i - j) / (4*k)\n",
        "                    mode_vals.append(this_val)\n",
        "\n",
        "                if -9999999 in info[\"modal\"]:\n",
        "                    mode_vals.append(0)\n",
        "\n",
        "                current = current.reshape([-1, 1])\n",
        "                filter_arr = self.filter_arr[mixed_counter]\n",
        "                current = current[filter_arr]\n",
        "\n",
        "                means = self.model[id_][1].means_.reshape((1, self.n_clusters))\n",
        "                stds = np.sqrt(self.model[id_][1].covariances_).reshape((1, self.n_clusters))\n",
        "                features = np.empty(shape=(len(current),self.n_clusters))\n",
        "                if ispositive == True:\n",
        "                    if id_ in positive_list:\n",
        "                        features = np.abs(current - means) / (4 * stds)\n",
        "                else:\n",
        "                    features = (current - means) / (4 * stds)\n",
        "\n",
        "                probs = self.model[id_][1].predict_proba(current.reshape([-1, 1]))\n",
        "\n",
        "                n_opts = sum(self.components[id_]) # 8\n",
        "                features = features[:, self.components[id_]]\n",
        "                probs = probs[:, self.components[id_]]\n",
        "\n",
        "                opt_sel = np.zeros(len(current), dtype='int')\n",
        "                for i in range(len(current)):\n",
        "                    pp = probs[i] + 1e-6\n",
        "                    pp = pp / sum(pp)\n",
        "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
        "                idx = np.arange((len(features)))\n",
        "                features = features[idx, opt_sel].reshape([-1, 1])\n",
        "                features = np.clip(features, -.99, .99)\n",
        "                probs_onehot = np.zeros_like(probs)\n",
        "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
        "                extra_bits = np.zeros([len(current), len(info['modal'])])\n",
        "                temp_probs_onehot = np.concatenate([extra_bits,probs_onehot], axis = 1)\n",
        "                final = np.zeros([len(data), 1 + probs_onehot.shape[1] + len(info['modal'])])\n",
        "                features_curser = 0\n",
        "                for idx, val in enumerate(data[:, id_]):\n",
        "                    if val in info['modal']:\n",
        "                        category_ = list(map(info['modal'].index, [val]))[0]\n",
        "                        final[idx, 0] = mode_vals[category_]\n",
        "                        final[idx, (category_+1)] = 1\n",
        "\n",
        "                    else:\n",
        "                        final[idx, 0] = features[features_curser]\n",
        "                        final[idx, (1+len(info['modal'])):] = temp_probs_onehot[features_curser][len(info['modal']):]\n",
        "                        features_curser = features_curser + 1\n",
        "\n",
        "                just_onehot = final[:,1:]\n",
        "                re_ordered_jhot= np.zeros_like(just_onehot)\n",
        "                n = just_onehot.shape[1]\n",
        "                col_sums = just_onehot.sum(axis=0)\n",
        "                largest_indices = np.argsort(-1*col_sums)[:n]\n",
        "                self.ordering.append(largest_indices)\n",
        "                for id,val in enumerate(largest_indices):\n",
        "                      re_ordered_jhot[:,id] = just_onehot[:,val]\n",
        "                final_features = final[:,0].reshape([-1, 1])\n",
        "                values += [final_features, re_ordered_jhot]\n",
        "                mixed_counter = mixed_counter + 1\n",
        "\n",
        "            else:\n",
        "                self.ordering.append(None)\n",
        "                col_t = np.zeros([len(data), info['size']])\n",
        "                idx = list(map(info['i2s'].index, current))\n",
        "                col_t[np.arange(len(data)), idx] = 1\n",
        "                values.append(col_t)\n",
        "\n",
        "        return np.concatenate(values, axis=1)\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        data_t = np.zeros([len(data), len(self.meta)])\n",
        "        invalid_ids = []\n",
        "        st = 0\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == \"continuous\":\n",
        "                if id_ not in self.general_columns:\n",
        "                  u = data[:, st]\n",
        "                  v = data[:, st + 1:st + 1 + np.sum(self.components[id_])]\n",
        "                  order = self.ordering[id_]\n",
        "                  v_re_ordered = np.zeros_like(v)\n",
        "\n",
        "                  for id,val in enumerate(order):\n",
        "                      v_re_ordered[:,val] = v[:,id]\n",
        "\n",
        "                  v = v_re_ordered\n",
        "\n",
        "                  u = np.clip(u, -1, 1)\n",
        "                  v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
        "                  v_t[:, self.components[id_]] = v\n",
        "                  v = v_t\n",
        "                  st += 1 + np.sum(self.components[id_])\n",
        "                  means = self.model[id_].means_.reshape([-1])\n",
        "                  stds = np.sqrt(self.model[id_].covariances_).reshape([-1])\n",
        "                  p_argmax = np.argmax(v, axis=1)\n",
        "                  std_t = stds[p_argmax]\n",
        "                  mean_t = means[p_argmax]\n",
        "                  tmp = u * 4 * std_t + mean_t\n",
        "\n",
        "                  for idx,val in enumerate(tmp):\n",
        "                     if (val < info[\"min\"]) | (val > info['max']):\n",
        "                         invalid_ids.append(idx)\n",
        "\n",
        "                  if id_ in self.non_categorical_columns:\n",
        "\n",
        "                    tmp = np.round(tmp)\n",
        "\n",
        "                  data_t[:, id_] = tmp\n",
        "\n",
        "                else:\n",
        "                  u = data[:, st]\n",
        "                  u = (u + 1) / 2\n",
        "                  u = np.clip(u, 0, 1)\n",
        "                  u = u * (info['max'] - info['min']) + info['min']\n",
        "                  if id_ in self.non_categorical_columns:\n",
        "                    data_t[:, id_] = np.round(u)\n",
        "                  else: data_t[:, id_] = u\n",
        "\n",
        "                  st += 1\n",
        "\n",
        "            elif info['type'] == \"mixed\":\n",
        "\n",
        "                u = data[:, st]\n",
        "                full_v = data[:,(st+1):(st+1)+len(info['modal'])+np.sum(self.components[id_])]\n",
        "                order = self.ordering[id_]\n",
        "                full_v_re_ordered = np.zeros_like(full_v)\n",
        "\n",
        "                for id,val in enumerate(order):\n",
        "                    full_v_re_ordered[:,val] = full_v[:,id]\n",
        "\n",
        "                full_v = full_v_re_ordered\n",
        "\n",
        "\n",
        "                mixed_v = full_v[:,:len(info['modal'])]\n",
        "                v = full_v[:,-np.sum(self.components[id_]):]\n",
        "\n",
        "                u = np.clip(u, -1, 1)\n",
        "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
        "                v_t[:, self.components[id_]] = v\n",
        "                v = np.concatenate([mixed_v,v_t], axis=1)\n",
        "\n",
        "                st += 1 + np.sum(self.components[id_]) + len(info['modal'])\n",
        "                means = self.model[id_][1].means_.reshape([-1])\n",
        "                stds = np.sqrt(self.model[id_][1].covariances_).reshape([-1])\n",
        "                p_argmax = np.argmax(v, axis=1)\n",
        "\n",
        "                result = np.zeros_like(u)\n",
        "\n",
        "                for idx in range(len(data)):\n",
        "                    if p_argmax[idx] < len(info['modal']):\n",
        "                        argmax_value = p_argmax[idx]\n",
        "                        result[idx] = float(list(map(info['modal'].__getitem__, [argmax_value]))[0])\n",
        "                    else:\n",
        "                        std_t = stds[(p_argmax[idx]-len(info['modal']))]\n",
        "                        mean_t = means[(p_argmax[idx]-len(info['modal']))]\n",
        "                        result[idx] = u[idx] * 4 * std_t + mean_t\n",
        "\n",
        "                for idx,val in enumerate(result):\n",
        "                     if (val < info[\"min\"]) | (val > info['max']):\n",
        "                         invalid_ids.append(idx)\n",
        "\n",
        "                data_t[:, id_] = result\n",
        "\n",
        "            else:\n",
        "                current = data[:, st:st + info['size']]\n",
        "                st += info['size']\n",
        "                idx = np.argmax(current, axis=1)\n",
        "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
        "\n",
        "\n",
        "        invalid_ids = np.unique(np.array(invalid_ids))\n",
        "        all_ids = np.arange(0,len(data))\n",
        "        valid_ids = list(set(all_ids) - set(invalid_ids))\n",
        "\n",
        "        return data_t[valid_ids],len(invalid_ids)\n",
        "\n",
        "\n",
        "class ImageTransformer():\n",
        "\n",
        "    def __init__(self, side):\n",
        "\n",
        "        self.height = side\n",
        "\n",
        "    def transform(self, data):\n",
        "\n",
        "        if self.height * self.height > len(data[0]):\n",
        "\n",
        "            padding = torch.zeros((len(data), self.height * self.height - len(data[0]))).to(data.device)\n",
        "            data = torch.cat([data, padding], axis=1)\n",
        "\n",
        "        return data.view(-1, 1, self.height, self.height)\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "\n",
        "        data = data.view(-1, self.height * self.height)\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "YjbBFHSOjHSE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from scipy import special\n",
        "import six\n",
        "\n",
        "########################\n",
        "# LOG-SPACE ARITHMETIC #\n",
        "########################\n",
        "\n",
        "\n",
        "def _log_add(logx, logy):\n",
        "  \"\"\"Add two numbers in the log space.\"\"\"\n",
        "  a, b = min(logx, logy), max(logx, logy)\n",
        "  if a == -np.inf:  # adding 0\n",
        "    return b\n",
        "  # Use exp(a) + exp(b) = (exp(a - b) + 1) * exp(b)\n",
        "  return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)\n",
        "\n",
        "\n",
        "def _log_sub(logx, logy):\n",
        "  \"\"\"Subtract two numbers in the log space. Answer must be non-negative.\"\"\"\n",
        "  if logx < logy:\n",
        "    raise ValueError(\"The result of subtraction must be non-negative.\")\n",
        "  if logy == -np.inf:  # subtracting 0\n",
        "    return logx\n",
        "  if logx == logy:\n",
        "    return -np.inf  # 0 is represented as -np.inf in the log space.\n",
        "\n",
        "  try:\n",
        "    # Use exp(x) - exp(y) = (exp(x - y) - 1) * exp(y).\n",
        "    return math.log(math.expm1(logx - logy)) + logy  # expm1(x) = exp(x) - 1\n",
        "  except OverflowError:\n",
        "    return logx\n",
        "\n",
        "\n",
        "def _log_print(logx):\n",
        "  \"\"\"Pretty print.\"\"\"\n",
        "  if logx < math.log(sys.float_info.max):\n",
        "    return \"{}\".format(math.exp(logx))\n",
        "  else:\n",
        "    return \"exp({})\".format(logx)\n",
        "\n",
        "\n",
        "def _compute_log_a_int(q, sigma, alpha):\n",
        "  \"\"\"Compute log(A_alpha) for integer alpha. 0 < q < 1.\"\"\"\n",
        "  assert isinstance(alpha, six.integer_types)\n",
        "\n",
        "  # Initialize with 0 in the log space.\n",
        "  log_a = -np.inf\n",
        "\n",
        "  for i in range(alpha + 1):\n",
        "    log_coef_i = (\n",
        "        math.log(special.binom(alpha, i)) + i * math.log(q) +\n",
        "        (alpha - i) * math.log(1 - q))\n",
        "\n",
        "    s = log_coef_i + (i * i - i) / (2 * (sigma**2))\n",
        "    log_a = _log_add(log_a, s)\n",
        "\n",
        "  return float(log_a)\n",
        "\n",
        "\n",
        "def _compute_log_a_frac(q, sigma, alpha):\n",
        "  \"\"\"Compute log(A_alpha) for fractional alpha. 0 < q < 1.\"\"\"\n",
        "  # The two parts of A_alpha, integrals over (-inf,z0] and [z0, +inf), are\n",
        "  # initialized to 0 in the log space:\n",
        "  log_a0, log_a1 = -np.inf, -np.inf\n",
        "  i = 0\n",
        "\n",
        "  z0 = sigma**2 * math.log(1 / q - 1) + .5\n",
        "\n",
        "  while True:  # do ... until loop\n",
        "    coef = special.binom(alpha, i)\n",
        "    log_coef = math.log(abs(coef))\n",
        "    j = alpha - i\n",
        "\n",
        "    log_t0 = log_coef + i * math.log(q) + j * math.log(1 - q)\n",
        "    log_t1 = log_coef + j * math.log(q) + i * math.log(1 - q)\n",
        "\n",
        "    log_e0 = math.log(.5) + _log_erfc((i - z0) / (math.sqrt(2) * sigma))\n",
        "    log_e1 = math.log(.5) + _log_erfc((z0 - j) / (math.sqrt(2) * sigma))\n",
        "\n",
        "    log_s0 = log_t0 + (i * i - i) / (2 * (sigma**2)) + log_e0\n",
        "    log_s1 = log_t1 + (j * j - j) / (2 * (sigma**2)) + log_e1\n",
        "\n",
        "    if coef > 0:\n",
        "      log_a0 = _log_add(log_a0, log_s0)\n",
        "      log_a1 = _log_add(log_a1, log_s1)\n",
        "    else:\n",
        "      log_a0 = _log_sub(log_a0, log_s0)\n",
        "      log_a1 = _log_sub(log_a1, log_s1)\n",
        "\n",
        "    i += 1\n",
        "    if max(log_s0, log_s1) < -30:\n",
        "      break\n",
        "\n",
        "  return _log_add(log_a0, log_a1)\n",
        "\n",
        "\n",
        "def _compute_log_a(q, sigma, alpha):\n",
        "  \"\"\"Compute log(A_alpha) for any positive finite alpha.\"\"\"\n",
        "  if float(alpha).is_integer():\n",
        "    return _compute_log_a_int(q, sigma, int(alpha))\n",
        "  else:\n",
        "    return _compute_log_a_frac(q, sigma, alpha)\n",
        "\n",
        "\n",
        "def _log_erfc(x):\n",
        "  \"\"\"Compute log(erfc(x)) with high accuracy for large x.\"\"\"\n",
        "  try:\n",
        "    return math.log(2) + special.log_ndtr(-x * 2**.5)\n",
        "  except NameError:\n",
        "    # If log_ndtr is not available, approximate as follows:\n",
        "    r = special.erfc(x)\n",
        "    if r == 0.0:\n",
        "      # Using the Laurent series at infinity for the tail of the erfc function:\n",
        "      #     erfc(x) ~ exp(-x^2-.5/x^2+.625/x^4)/(x*pi^.5)\n",
        "      # To verify in Mathematica:\n",
        "      #     Series[Log[Erfc[x]] + Log[x] + Log[Pi]/2 + x^2, {x, Infinity, 6}]\n",
        "      return (-math.log(math.pi) / 2 - math.log(x) - x**2 - .5 * x**-2 +\n",
        "              .625 * x**-4 - 37. / 24. * x**-6 + 353. / 64. * x**-8)\n",
        "    else:\n",
        "      return math.log(r)\n",
        "\n",
        "\n",
        "def _compute_delta(orders, rdp, eps):\n",
        "  \"\"\"Compute delta given a list of RDP values and target epsilon.\n",
        "\n",
        "  Args:\n",
        "    orders: An array (or a scalar) of orders.\n",
        "    rdp: A list (or a scalar) of RDP guarantees.\n",
        "    eps: The target epsilon.\n",
        "\n",
        "  Returns:\n",
        "    Pair of (delta, optimal_order).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If input is malformed.\n",
        "\n",
        "  \"\"\"\n",
        "  orders_vec = np.atleast_1d(orders)\n",
        "  rdp_vec = np.atleast_1d(rdp)\n",
        "\n",
        "  if len(orders_vec) != len(rdp_vec):\n",
        "    raise ValueError(\"Input lists must have the same length.\")\n",
        "\n",
        "  deltas = np.exp((rdp_vec - eps) * (orders_vec - 1))\n",
        "  idx_opt = np.argmin(deltas)\n",
        "  return min(deltas[idx_opt], 1.), orders_vec[idx_opt]\n",
        "\n",
        "\n",
        "def _compute_eps(orders, rdp, delta):\n",
        "  \"\"\"Compute epsilon given a list of RDP values and target delta.\n",
        "\n",
        "  Args:\n",
        "    orders: An array (or a scalar) of orders.\n",
        "    rdp: A list (or a scalar) of RDP guarantees.\n",
        "    delta: The target delta.\n",
        "\n",
        "  Returns:\n",
        "    Pair of (eps, optimal_order).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If input is malformed.\n",
        "\n",
        "  \"\"\"\n",
        "  orders_vec = np.atleast_1d(orders)\n",
        "  rdp_vec = np.atleast_1d(rdp)\n",
        "\n",
        "  if len(orders_vec) != len(rdp_vec):\n",
        "    raise ValueError(\"Input lists must have the same length.\")\n",
        "\n",
        "  eps = rdp_vec - math.log(delta) / (orders_vec - 1)\n",
        "\n",
        "  idx_opt = np.nanargmin(eps)  # Ignore NaNs\n",
        "  return eps[idx_opt], orders_vec[idx_opt]\n",
        "\n",
        "\n",
        "def _compute_rdp(q, sigma, alpha):\n",
        "  \"\"\"Compute RDP of the Sampled Gaussian mechanism at order alpha.\n",
        "\n",
        "  Args:\n",
        "    q: The sampling rate.\n",
        "    sigma: The std of the additive Gaussian noise.\n",
        "    alpha: The order at which RDP is computed.\n",
        "\n",
        "  Returns:\n",
        "    RDP at alpha, can be np.inf.\n",
        "  \"\"\"\n",
        "  if q == 0:\n",
        "    return 0\n",
        "\n",
        "  if q == 1.:\n",
        "    return alpha / (2 * sigma**2)\n",
        "\n",
        "  if np.isinf(alpha):\n",
        "    return np.inf\n",
        "\n",
        "  return _compute_log_a(q, sigma, alpha) / (alpha - 1)\n",
        "\n",
        "\n",
        "def compute_rdp(q, noise_multiplier, steps, orders):\n",
        "  \"\"\"Compute RDP of the Sampled Gaussian Mechanism.\n",
        "\n",
        "  Args:\n",
        "    q: The sampling rate.\n",
        "    noise_multiplier: The ratio of the standard deviation of the Gaussian noise\n",
        "        to the l2-sensitivity of the function to which it is added.\n",
        "    steps: The number of steps.\n",
        "    orders: An array (or a scalar) of RDP orders.\n",
        "\n",
        "  Returns:\n",
        "    The RDPs at all orders, can be np.inf.\n",
        "  \"\"\"\n",
        "  if np.isscalar(orders):\n",
        "    rdp = _compute_rdp(q, noise_multiplier, orders)\n",
        "  else:\n",
        "    rdp = np.array([_compute_rdp(q, noise_multiplier, order)\n",
        "                    for order in orders])\n",
        "\n",
        "  return rdp * steps\n",
        "\n",
        "\n",
        "def get_privacy_spent(orders, rdp, target_eps=None, target_delta=None):\n",
        "  \"\"\"Compute delta (or eps) for given eps (or delta) from RDP values.\n",
        "\n",
        "  Args:\n",
        "    orders: An array (or a scalar) of RDP orders.\n",
        "    rdp: An array of RDP values. Must be of the same length as the orders list.\n",
        "    target_eps: If not None, the epsilon for which we compute the corresponding\n",
        "              delta.\n",
        "    target_delta: If not None, the delta for which we compute the corresponding\n",
        "              epsilon. Exactly one of target_eps and target_delta must be None.\n",
        "\n",
        "  Returns:\n",
        "    eps, delta, opt_order.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If target_eps and target_delta are messed up.\n",
        "  \"\"\"\n",
        "  if target_eps is None and target_delta is None:\n",
        "    raise ValueError(\n",
        "        \"Exactly one out of eps and delta must be None. (Both are).\")\n",
        "\n",
        "  if target_eps is not None and target_delta is not None:\n",
        "    raise ValueError(\n",
        "        \"Exactly one out of eps and delta must be None. (None is).\")\n",
        "\n",
        "  if target_eps is not None:\n",
        "    delta, opt_order = _compute_delta(orders, rdp, target_eps)\n",
        "    return target_eps, delta, opt_order\n",
        "  else:\n",
        "    eps, opt_order = _compute_eps(orders, rdp, target_delta)\n",
        "    return eps, target_delta, opt_order\n",
        "\n",
        "\n",
        "def compute_rdp_from_ledger(ledger, orders):\n",
        "  \"\"\"Compute RDP of Sampled Gaussian Mechanism from ledger.\n",
        "\n",
        "  Args:\n",
        "    ledger: A formatted privacy ledger.\n",
        "    orders: An array (or a scalar) of RDP orders.\n",
        "\n",
        "  Returns:\n",
        "    RDP at all orders, can be np.inf.\n",
        "  \"\"\"\n",
        "  total_rdp = np.zeros_like(orders, dtype=float)\n",
        "  for sample in ledger:\n",
        "    # Compute equivalent z from l2_clip_bounds and noise stddevs in sample.\n",
        "    # See https://arxiv.org/pdf/1812.06210.pdf for derivation of this formula.\n",
        "    effective_z = sum([\n",
        "        (q.noise_stddev / q.l2_norm_bound)**-2 for q in sample.queries])**-0.5\n",
        "    total_rdp += compute_rdp(\n",
        "        sample.selection_probability, effective_z, 1, orders)\n",
        "  return total_rdp"
      ],
      "metadata": {
        "id": "JSjRan8WjULT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import (Dropout, LeakyReLU, Linear, Module, ReLU, Sequential,\n",
        "Conv2d, ConvTranspose2d, Sigmoid, init, BCELoss, CrossEntropyLoss,SmoothL1Loss,LayerNorm)\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class Classifier(Module):\n",
        "    def __init__(self,input_dim, dis_dims,st_ed):\n",
        "        super(Classifier,self).__init__()\n",
        "        dim = input_dim-(st_ed[1]-st_ed[0])\n",
        "        seq = []\n",
        "        self.str_end = st_ed\n",
        "        for item in list(dis_dims):\n",
        "            seq += [\n",
        "                Linear(dim, item),\n",
        "                LeakyReLU(0.2),\n",
        "                Dropout(0.5)\n",
        "            ]\n",
        "            dim = item\n",
        "\n",
        "        if (st_ed[1]-st_ed[0])==1:\n",
        "            seq += [Linear(dim, 1)]\n",
        "\n",
        "        elif (st_ed[1]-st_ed[0])==2:\n",
        "            seq += [Linear(dim, 1),Sigmoid()]\n",
        "        else:\n",
        "            seq += [Linear(dim,(st_ed[1]-st_ed[0]))]\n",
        "\n",
        "        self.seq = Sequential(*seq)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        label=None\n",
        "\n",
        "        if (self.str_end[1]-self.str_end[0])==1:\n",
        "            label = input[:, self.str_end[0]:self.str_end[1]]\n",
        "        else:\n",
        "            label = torch.argmax(input[:, self.str_end[0]:self.str_end[1]], axis=-1)\n",
        "\n",
        "        new_imp = torch.cat((input[:,:self.str_end[0]],input[:,self.str_end[1]:]),1)\n",
        "\n",
        "        if ((self.str_end[1]-self.str_end[0])==2) | ((self.str_end[1]-self.str_end[0])==1):\n",
        "            return self.seq(new_imp).view(-1), label\n",
        "        else:\n",
        "            return self.seq(new_imp), label\n",
        "\n",
        "def apply_activate(data, output_info):\n",
        "    data_t = []\n",
        "    st = 0\n",
        "    for item in output_info:\n",
        "        if item[1] == 'tanh':\n",
        "            ed = st + item[0]\n",
        "            data_t.append(torch.tanh(data[:, st:ed]))\n",
        "            st = ed\n",
        "        elif item[1] == 'softmax':\n",
        "            ed = st + item[0]\n",
        "            data_t.append(F.gumbel_softmax(data[:, st:ed], tau=0.2))\n",
        "            st = ed\n",
        "    return torch.cat(data_t, dim=1)\n",
        "\n",
        "def get_st_ed(target_col_index,output_info):\n",
        "    st = 0\n",
        "    c= 0\n",
        "    tc= 0\n",
        "\n",
        "    for item in output_info:\n",
        "        if c==target_col_index:\n",
        "            break\n",
        "        if item[1]=='tanh':\n",
        "            st += item[0]\n",
        "            if item[2] == 'yes_g':\n",
        "                c+=1\n",
        "        elif item[1] == 'softmax':\n",
        "            st += item[0]\n",
        "            c+=1\n",
        "        tc+=1\n",
        "\n",
        "    ed= st+output_info[tc][0]\n",
        "\n",
        "    return (st,ed)\n",
        "\n",
        "def random_choice_prob_index_sampling(probs,col_idx):\n",
        "    option_list = []\n",
        "    for i in col_idx:\n",
        "        pp = probs[i]\n",
        "        option_list.append(np.random.choice(np.arange(len(probs[i])), p=pp))\n",
        "\n",
        "    return np.array(option_list).reshape(col_idx.shape)\n",
        "\n",
        "def random_choice_prob_index(a, axis=1):\n",
        "    r = np.expand_dims(np.random.rand(a.shape[1 - axis]), axis=axis)\n",
        "    return (a.cumsum(axis=axis) > r).argmax(axis=axis)\n",
        "\n",
        "def maximum_interval(output_info):\n",
        "    max_interval = 0\n",
        "    for item in output_info:\n",
        "        max_interval = max(max_interval, item[0])\n",
        "    return max_interval\n",
        "\n",
        "class Cond(object):\n",
        "    def __init__(self, data, output_info):\n",
        "\n",
        "        self.model = []\n",
        "        st = 0\n",
        "        counter = 0\n",
        "        for item in output_info:\n",
        "\n",
        "            if item[1] == 'tanh':\n",
        "                st += item[0]\n",
        "                continue\n",
        "            elif item[1] == 'softmax':\n",
        "                ed = st + item[0]\n",
        "                counter += 1\n",
        "                self.model.append(np.argmax(data[:, st:ed], axis=-1))\n",
        "                st = ed\n",
        "\n",
        "        self.interval = []\n",
        "        self.n_col = 0\n",
        "        self.n_opt = 0\n",
        "        st = 0\n",
        "        self.p = np.zeros((counter, maximum_interval(output_info)))\n",
        "        self.p_sampling = []\n",
        "        for item in output_info:\n",
        "            if item[1] == 'tanh':\n",
        "                st += item[0]\n",
        "                continue\n",
        "            elif item[1] == 'softmax':\n",
        "                ed = st + item[0]\n",
        "                tmp = np.sum(data[:, st:ed], axis=0)\n",
        "                tmp_sampling = np.sum(data[:, st:ed], axis=0)\n",
        "                tmp = np.log(tmp + 1)\n",
        "                tmp = tmp / np.sum(tmp)\n",
        "                tmp_sampling = tmp_sampling / np.sum(tmp_sampling)\n",
        "                self.p_sampling.append(tmp_sampling)\n",
        "                self.p[self.n_col, :item[0]] = tmp\n",
        "                self.interval.append((self.n_opt, item[0]))\n",
        "                self.n_opt += item[0]\n",
        "                self.n_col += 1\n",
        "                st = ed\n",
        "\n",
        "        self.interval = np.asarray(self.interval)\n",
        "\n",
        "    def sample_train(self, batch):\n",
        "        if self.n_col == 0:\n",
        "            return None\n",
        "        batch = batch\n",
        "\n",
        "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
        "\n",
        "        vec = np.zeros((batch, self.n_opt), dtype='float32')\n",
        "        mask = np.zeros((batch, self.n_col), dtype='float32')\n",
        "        mask[np.arange(batch), idx] = 1\n",
        "        opt1prime = random_choice_prob_index(self.p[idx])\n",
        "        for i in np.arange(batch):\n",
        "            vec[i, self.interval[idx[i], 0] + opt1prime[i]] = 1\n",
        "\n",
        "        return vec, mask, idx, opt1prime\n",
        "\n",
        "    def sample(self, batch):\n",
        "        if self.n_col == 0:\n",
        "            return None\n",
        "        batch = batch\n",
        "\n",
        "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
        "\n",
        "        vec = np.zeros((batch, self.n_opt), dtype='float32')\n",
        "        opt1prime = random_choice_prob_index_sampling(self.p_sampling,idx)\n",
        "\n",
        "        for i in np.arange(batch):\n",
        "            vec[i, self.interval[idx[i], 0] + opt1prime[i]] = 1\n",
        "\n",
        "        return vec\n",
        "\n",
        "def cond_loss(data, output_info, c, m):\n",
        "    loss = []\n",
        "    st = 0\n",
        "    st_c = 0\n",
        "    for item in output_info:\n",
        "        if item[1] == 'tanh':\n",
        "            st += item[0]\n",
        "            continue\n",
        "\n",
        "        elif item[1] == 'softmax':\n",
        "            ed = st + item[0]\n",
        "            ed_c = st_c + item[0]\n",
        "            tmp = F.cross_entropy(\n",
        "            data[:, st:ed],\n",
        "            torch.argmax(c[:, st_c:ed_c], dim=1),\n",
        "            reduction='none')\n",
        "            loss.append(tmp)\n",
        "            st = ed\n",
        "            st_c = ed_c\n",
        "\n",
        "    loss = torch.stack(loss, dim=1)\n",
        "    return (loss * m).sum() / data.size()[0]\n",
        "\n",
        "class Sampler(object):\n",
        "    def __init__(self, data, output_info):\n",
        "        super(Sampler, self).__init__()\n",
        "        self.data = data\n",
        "        self.model = []\n",
        "        self.n = len(data)\n",
        "        st = 0\n",
        "        for item in output_info:\n",
        "            if item[1] == 'tanh':\n",
        "                st += item[0]\n",
        "                continue\n",
        "            elif item[1] == 'softmax':\n",
        "                ed = st + item[0]\n",
        "                tmp = []\n",
        "                for j in range(item[0]):\n",
        "                    tmp.append(np.nonzero(data[:, st + j])[0])\n",
        "                self.model.append(tmp)\n",
        "                st = ed\n",
        "\n",
        "    def sample(self, n, col, opt):\n",
        "        if col is None:\n",
        "            idx = np.random.choice(np.arange(self.n), n)\n",
        "            return self.data[idx]\n",
        "        idx = []\n",
        "        for c, o in zip(col, opt):\n",
        "            idx.append(np.random.choice(self.model[c][o]))\n",
        "        return self.data[idx]\n",
        "\n",
        "class Discriminator(Module):\n",
        "    def __init__(self, side, layers):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.side = side\n",
        "        info = len(layers)-2\n",
        "        self.seq = Sequential(*layers)\n",
        "        self.seq_info = Sequential(*layers[:info])\n",
        "\n",
        "    def forward(self, input):\n",
        "        return (self.seq(input)), self.seq_info(input)\n",
        "\n",
        "class Generator(Module):\n",
        "    def __init__(self, side, layers):\n",
        "        super(Generator, self).__init__()\n",
        "        self.side = side\n",
        "        self.seq = Sequential(*layers)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        return self.seq(input_)\n",
        "\n",
        "def determine_layers_disc(side, num_channels):\n",
        "    assert side >= 4 and side <= 64\n",
        "\n",
        "    layer_dims = [(1, side), (num_channels, side // 2)]\n",
        "\n",
        "    while layer_dims[-1][1] > 3 and len(layer_dims) < 4:\n",
        "        layer_dims.append((layer_dims[-1][0] * 2, layer_dims[-1][1] // 2))\n",
        "\n",
        "    layerNorms = []\n",
        "    num_c = num_channels\n",
        "    num_s = side / 2\n",
        "    for l in range(len(layer_dims) - 1):\n",
        "        layerNorms.append([int(num_c), int(num_s), int(num_s)])\n",
        "        num_c = num_c * 2\n",
        "        num_s = num_s / 2\n",
        "\n",
        "    layers_D = []\n",
        "\n",
        "    for prev, curr, ln in zip(layer_dims, layer_dims[1:], layerNorms):\n",
        "        layers_D += [\n",
        "            Conv2d(prev[0], curr[0], 4, 2, 1, bias=False),\n",
        "            LayerNorm(ln),\n",
        "            LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "    layers_D += [Conv2d(layer_dims[-1][0], 1, layer_dims[-1][1], 1, 0), ReLU(True)]\n",
        "\n",
        "    return layers_D\n",
        "\n",
        "def determine_layers_gen(side, random_dim, num_channels):\n",
        "    assert side >= 4 and side <= 64\n",
        "\n",
        "    layer_dims = [(1, side), (num_channels, side // 2)]\n",
        "\n",
        "    while layer_dims[-1][1] > 3 and len(layer_dims) < 4:\n",
        "        layer_dims.append((layer_dims[-1][0] * 2, layer_dims[-1][1] // 2))\n",
        "\n",
        "    layerNorms = []\n",
        "\n",
        "    num_c = num_channels * (2 ** (len(layer_dims) - 2))\n",
        "    num_s = int(side / (2 ** (len(layer_dims) - 1)))\n",
        "    for l in range(len(layer_dims) - 1):\n",
        "        layerNorms.append([int(num_c), int(num_s), int(num_s)])\n",
        "        num_c = num_c / 2\n",
        "        num_s = num_s * 2\n",
        "\n",
        "    layers_G = [ConvTranspose2d(random_dim, layer_dims[-1][0], layer_dims[-1][1], 1, 0, output_padding=0, bias=False)]\n",
        "\n",
        "    for prev, curr, ln in zip(reversed(layer_dims), reversed(layer_dims[:-1]), layerNorms):\n",
        "        layers_G += [LayerNorm(ln), ReLU(True), ConvTranspose2d(prev[0], curr[0], 4, 2, 1, output_padding=0, bias=True)]\n",
        "    return layers_G\n",
        "\n",
        "def slerp(val, low, high):\n",
        "    low_norm = low/torch.norm(low, dim=1, keepdim=True)\n",
        "    high_norm = high/torch.norm(high, dim=1, keepdim=True)\n",
        "    omega = torch.acos((low_norm*high_norm).sum(1)).view(val.size(0), 1)\n",
        "    so = torch.sin(omega)\n",
        "    res = (torch.sin((1.0-val)*omega)/so)*low + (torch.sin(val*omega)/so) * high\n",
        "\n",
        "    return res\n",
        "\n",
        "def calc_gradient_penalty_slerp(netD, real_data, fake_data, transformer, device='cpu', lambda_=10):\n",
        "    batchsize = real_data.shape[0]\n",
        "    alpha = torch.rand(batchsize, 1,  device=device)\n",
        "    interpolates = slerp(alpha, real_data, fake_data)\n",
        "    interpolates = interpolates.to(device)\n",
        "    interpolates = transformer.transform(interpolates)\n",
        "    interpolates = torch.autograd.Variable(interpolates, requires_grad=True)\n",
        "    disc_interpolates,_ = netD(interpolates)\n",
        "\n",
        "    gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                                  grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
        "                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradients_norm = gradients.norm(2, dim=1)\n",
        "    gradient_penalty = ((gradients_norm - 1) ** 2).mean() * lambda_\n",
        "\n",
        "    return gradient_penalty\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "\n",
        "class CTABGANSynthesizer():\n",
        "    def __init__(self,\n",
        "                 class_dim=(256, 256, 256, 256),\n",
        "                 random_dim=100,\n",
        "                 num_channels=64,\n",
        "                 l2scale=1e-5,\n",
        "                 batch_size=500,\n",
        "                 epochs=150):\n",
        "\n",
        "\n",
        "        self.random_dim = random_dim\n",
        "        self.class_dim = class_dim\n",
        "        self.num_channels = num_channels\n",
        "        self.dside = None\n",
        "        self.gside = None\n",
        "        self.l2scale = l2scale\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def fit(self, train_data=pd.DataFrame, categorical=[], mixed={}, general=[], non_categorical=[], type={}):\n",
        "\n",
        "        problem_type = None\n",
        "        target_index=None\n",
        "        if type:\n",
        "            problem_type = list(type.keys())[0]\n",
        "            if problem_type:\n",
        "                target_index = train_data.columns.get_loc(type[problem_type])\n",
        "\n",
        "        self.transformer = DataTransformer(train_data=train_data, categorical_list=categorical, mixed_dict=mixed, general_list=general, non_categorical_list=non_categorical)\n",
        "        self.transformer.fit()\n",
        "        train_data = self.transformer.transform(train_data.values)\n",
        "        data_sampler = Sampler(train_data, self.transformer.output_info)\n",
        "        data_dim = self.transformer.output_dim\n",
        "        self.cond_generator = Cond(train_data, self.transformer.output_info)\n",
        "\n",
        "        sides = [4, 8, 16, 24, 32, 64]\n",
        "        col_size_d = data_dim + self.cond_generator.n_opt\n",
        "        for i in sides:\n",
        "            if i * i >= col_size_d:\n",
        "                self.dside = i\n",
        "                break\n",
        "\n",
        "        sides = [4, 8, 16, 24, 32, 64]\n",
        "        col_size_g = data_dim\n",
        "        for i in sides:\n",
        "            if i * i >= col_size_g:\n",
        "                self.gside = i\n",
        "                break\n",
        "\n",
        "\n",
        "        layers_G = determine_layers_gen(self.gside, self.random_dim+self.cond_generator.n_opt, self.num_channels)\n",
        "        layers_D = determine_layers_disc(self.dside, self.num_channels)\n",
        "\n",
        "        self.generator = Generator(self.gside, layers_G).to(self.device)\n",
        "        discriminator = Discriminator(self.dside, layers_D).to(self.device)\n",
        "        optimizer_params = dict(lr=2e-4, betas=(0.5, 0.9), eps=1e-3, weight_decay=self.l2scale)\n",
        "        optimizerG = Adam(self.generator.parameters(), **optimizer_params)\n",
        "        optimizerD = Adam(discriminator.parameters(), **optimizer_params)\n",
        "\n",
        "        st_ed = None\n",
        "        classifier=None\n",
        "        optimizerC= None\n",
        "        if target_index != None:\n",
        "            st_ed= get_st_ed(target_index,self.transformer.output_info)\n",
        "            classifier = Classifier(data_dim,self.class_dim,st_ed).to(self.device)\n",
        "            optimizerC = optim.Adam(classifier.parameters(),**optimizer_params)\n",
        "\n",
        "\n",
        "        self.generator.apply(weights_init)\n",
        "        discriminator.apply(weights_init)\n",
        "\n",
        "        self.Gtransformer = ImageTransformer(self.gside)\n",
        "        self.Dtransformer = ImageTransformer(self.dside)\n",
        "\n",
        "        epsilon = 0\n",
        "        epoch = 0\n",
        "        steps = 0\n",
        "        ci = 5\n",
        "\n",
        "        steps_per_epoch = max(1, len(train_data) // self.batch_size)\n",
        "        for i in tqdm(range(self.epochs)):\n",
        "            for id_ in range(steps_per_epoch):\n",
        "\n",
        "\n",
        "                for _ in range(ci):\n",
        "                    noisez = torch.randn(self.batch_size, self.random_dim, device=self.device)\n",
        "                    condvec = self.cond_generator.sample_train(self.batch_size)\n",
        "\n",
        "                    c, m, col, opt = condvec\n",
        "                    c = torch.from_numpy(c).to(self.device)\n",
        "                    m = torch.from_numpy(m).to(self.device)\n",
        "                    noisez = torch.cat([noisez, c], dim=1)\n",
        "                    noisez =  noisez.view(self.batch_size,self.random_dim+self.cond_generator.n_opt,1,1)\n",
        "\n",
        "                    perm = np.arange(self.batch_size)\n",
        "                    np.random.shuffle(perm)\n",
        "                    real = data_sampler.sample(self.batch_size, col[perm], opt[perm])\n",
        "                    c_perm = c[perm]\n",
        "\n",
        "                    real = torch.from_numpy(real.astype('float32')).to(self.device)\n",
        "\n",
        "                    fake = self.generator(noisez)\n",
        "                    faket = self.Gtransformer.inverse_transform(fake)\n",
        "                    fakeact = apply_activate(faket, self.transformer.output_info)\n",
        "\n",
        "                    fake_cat = torch.cat([fakeact, c], dim=1)\n",
        "                    real_cat = torch.cat([real, c_perm], dim=1)\n",
        "\n",
        "                    real_cat_d = self.Dtransformer.transform(real_cat)\n",
        "                    fake_cat_d = self.Dtransformer.transform(fake_cat)\n",
        "\n",
        "                    optimizerD.zero_grad()\n",
        "\n",
        "                    d_real,_ = discriminator(real_cat_d)\n",
        "\n",
        "\n",
        "                    d_real = -torch.mean(d_real)\n",
        "                    d_real.backward()\n",
        "\n",
        "\n",
        "                    d_fake,_ = discriminator(fake_cat_d)\n",
        "\n",
        "                    d_fake = torch.mean(d_fake)\n",
        "\n",
        "                    d_fake.backward()\n",
        "\n",
        "                    pen = calc_gradient_penalty_slerp(discriminator, real_cat, fake_cat,  self.Dtransformer , self.device)\n",
        "\n",
        "                    pen.backward()\n",
        "\n",
        "                    optimizerD.step()\n",
        "\n",
        "                noisez = torch.randn(self.batch_size, self.random_dim, device=self.device)\n",
        "\n",
        "                condvec = self.cond_generator.sample_train(self.batch_size)\n",
        "\n",
        "                c, m, col, opt = condvec\n",
        "                c = torch.from_numpy(c).to(self.device)\n",
        "                m = torch.from_numpy(m).to(self.device)\n",
        "                noisez = torch.cat([noisez, c], dim=1)\n",
        "                noisez =  noisez.view(self.batch_size,self.random_dim+self.cond_generator.n_opt,1,1)\n",
        "\n",
        "                optimizerG.zero_grad()\n",
        "\n",
        "                fake = self.generator(noisez)\n",
        "                faket = self.Gtransformer.inverse_transform(fake)\n",
        "                fakeact = apply_activate(faket, self.transformer.output_info)\n",
        "\n",
        "                fake_cat = torch.cat([fakeact, c], dim=1)\n",
        "                fake_cat = self.Dtransformer.transform(fake_cat)\n",
        "\n",
        "                y_fake,info_fake = discriminator(fake_cat)\n",
        "\n",
        "                cross_entropy = cond_loss(faket, self.transformer.output_info, c, m)\n",
        "\n",
        "                _,info_real = discriminator(real_cat_d)\n",
        "\n",
        "\n",
        "                g = -torch.mean(y_fake) + cross_entropy\n",
        "                g.backward(retain_graph=True)\n",
        "                loss_mean = torch.norm(torch.mean(info_fake.view(self.batch_size,-1), dim=0) - torch.mean(info_real.view(self.batch_size,-1), dim=0), 1)\n",
        "                loss_std = torch.norm(torch.std(info_fake.view(self.batch_size,-1), dim=0) - torch.std(info_real.view(self.batch_size,-1), dim=0), 1)\n",
        "                loss_info = loss_mean + loss_std\n",
        "                loss_info.backward()\n",
        "                optimizerG.step()\n",
        "\n",
        "\n",
        "                if problem_type:\n",
        "\n",
        "                    fake = self.generator(noisez)\n",
        "\n",
        "                    faket = self.Gtransformer.inverse_transform(fake)\n",
        "\n",
        "                    fakeact = apply_activate(faket, self.transformer.output_info)\n",
        "\n",
        "                    real_pre, real_label = classifier(real)\n",
        "                    fake_pre, fake_label = classifier(fakeact)\n",
        "\n",
        "                    c_loss = CrossEntropyLoss()\n",
        "\n",
        "                    if (st_ed[1] - st_ed[0])==1:\n",
        "                        c_loss= SmoothL1Loss()\n",
        "                        real_label = real_label.type_as(real_pre)\n",
        "                        fake_label = fake_label.type_as(fake_pre)\n",
        "                        real_label = torch.reshape(real_label,real_pre.size())\n",
        "                        fake_label = torch.reshape(fake_label,fake_pre.size())\n",
        "\n",
        "\n",
        "                    elif (st_ed[1] - st_ed[0])==2:\n",
        "                        c_loss = BCELoss()\n",
        "                        real_label = real_label.type_as(real_pre)\n",
        "                        fake_label = fake_label.type_as(fake_pre)\n",
        "\n",
        "                    loss_cc = c_loss(real_pre, real_label)\n",
        "                    loss_cg = c_loss(fake_pre, fake_label)\n",
        "\n",
        "                    optimizerG.zero_grad()\n",
        "                    loss_cg.backward()\n",
        "                    optimizerG.step()\n",
        "\n",
        "                    optimizerC.zero_grad()\n",
        "                    loss_cc.backward()\n",
        "                    optimizerC.step()\n",
        "\n",
        "            epoch += 1\n",
        "\n",
        "\n",
        "\n",
        "    def sample(self, n):\n",
        "\n",
        "        self.generator.eval()\n",
        "\n",
        "        output_info = self.transformer.output_info\n",
        "        steps = n // self.batch_size + 1\n",
        "\n",
        "        data = []\n",
        "\n",
        "        for i in range(steps):\n",
        "            noisez = torch.randn(self.batch_size, self.random_dim, device=self.device)\n",
        "            condvec = self.cond_generator.sample(self.batch_size)\n",
        "            c = condvec\n",
        "            c = torch.from_numpy(c).to(self.device)\n",
        "            noisez = torch.cat([noisez, c], dim=1)\n",
        "            noisez =  noisez.view(self.batch_size,self.random_dim+self.cond_generator.n_opt,1,1)\n",
        "\n",
        "            fake = self.generator(noisez)\n",
        "            faket = self.Gtransformer.inverse_transform(fake)\n",
        "            fakeact = apply_activate(faket,output_info)\n",
        "            data.append(fakeact.detach().cpu().numpy())\n",
        "\n",
        "        data = np.concatenate(data, axis=0)\n",
        "        result,resample = self.transformer.inverse_transform(data)\n",
        "\n",
        "        while len(result) < n:\n",
        "            data_resample = []\n",
        "            steps_left = resample// self.batch_size + 1\n",
        "\n",
        "            for i in range(steps_left):\n",
        "                noisez = torch.randn(self.batch_size, self.random_dim, device=self.device)\n",
        "                condvec = self.cond_generator.sample(self.batch_size)\n",
        "                c = condvec\n",
        "                c = torch.from_numpy(c).to(self.device)\n",
        "                noisez = torch.cat([noisez, c], dim=1)\n",
        "                noisez =  noisez.view(self.batch_size,self.random_dim+self.cond_generator.n_opt,1,1)\n",
        "\n",
        "                fake = self.generator(noisez)\n",
        "                faket = self.Gtransformer.inverse_transform(fake)\n",
        "                fakeact = apply_activate(faket, output_info)\n",
        "                data_resample.append(fakeact.detach().cpu().numpy())\n",
        "\n",
        "            data_resample = np.concatenate(data_resample, axis=0)\n",
        "\n",
        "            res,resample = self.transformer.inverse_transform(data_resample)\n",
        "            result  = np.concatenate([result,res],axis=0)\n",
        "\n",
        "        return result[0:n]\n"
      ],
      "metadata": {
        "id": "u30ZiAzRjUIk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ek67A0cbijJp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class CTABGAN():\n",
        "\n",
        "    def __init__(self,\n",
        "                 raw_csv_path = \"/content/drive/MyDrive/Colab Notebooks/Dataset/Adult.csv\",\n",
        "                 test_ratio = 0.20,\n",
        "                 categorical_columns = [ 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country', 'income'],\n",
        "                 log_columns = [],\n",
        "                 mixed_columns= {'capital-loss':[0.0],'capital-gain':[0.0]},\n",
        "                 general_columns = [\"age\"],\n",
        "                 non_categorical_columns = [],\n",
        "                 integer_columns = ['age', 'fnlwgt','capital-gain', 'capital-loss','hours-per-week'],\n",
        "                 problem_type= {\"Classification\": \"income\"}):\n",
        "\n",
        "        self.__name__ = 'CTABGAN'\n",
        "\n",
        "        self.synthesizer = CTABGANSynthesizer()\n",
        "        self.raw_df = pd.read_csv(raw_csv_path)\n",
        "        self.test_ratio = test_ratio\n",
        "        self.categorical_columns = categorical_columns\n",
        "        self.log_columns = log_columns\n",
        "        self.mixed_columns = mixed_columns\n",
        "        self.general_columns = general_columns\n",
        "        self.non_categorical_columns = non_categorical_columns\n",
        "        self.integer_columns = integer_columns\n",
        "        self.problem_type = problem_type\n",
        "\n",
        "    def fit(self):\n",
        "\n",
        "        start_time = time.time()\n",
        "        self.data_prep = DataPrep(self.raw_df,self.categorical_columns,self.log_columns,self.mixed_columns,self.general_columns,self.non_categorical_columns,self.integer_columns,self.problem_type,self.test_ratio)\n",
        "        self.synthesizer.fit(train_data=self.data_prep.df, categorical = self.data_prep.column_types[\"categorical\"], mixed = self.data_prep.column_types[\"mixed\"],\n",
        "        general = self.data_prep.column_types[\"general\"], non_categorical = self.data_prep.column_types[\"non_categorical\"], type=self.problem_type)\n",
        "        end_time = time.time()\n",
        "        print('Finished training in',end_time-start_time,\" seconds.\")\n",
        "\n",
        "\n",
        "    def generate_samples(self):\n",
        "\n",
        "        sample = self.synthesizer.sample(len(self.raw_df))\n",
        "        sample_df = self.data_prep.inverse_prep(sample)\n",
        "\n",
        "        return sample_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  ..\n",
        "ctabgan = CTABGAN()\n",
        "ctabgan.fit()\n",
        "sample_df = ctabgan.generate_samples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "y9FY21tpi2x4",
        "outputId": "1fff2faf-5c3a-427a-c4f8-16a7671ba647"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/150 [01:10<2:55:00, 70.47s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1c277a25900f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mctabgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCTABGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mctabgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctabgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-fa8a1d196542>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_prep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataPrep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_categorical_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         self.synthesizer.fit(train_data=self.data_prep.df, categorical = self.data_prep.column_types[\"categorical\"], mixed = self.data_prep.column_types[\"mixed\"],\n\u001b[0m\u001b[1;32m     39\u001b[0m         general = self.data_prep.column_types[\"general\"], non_categorical = self.data_prep.column_types[\"non_categorical\"], type=self.problem_type)\n\u001b[1;32m     40\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-27233b7eda73>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, categorical, mixed, general, non_categorical, type)\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0md_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                     \u001b[0mpen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_gradient_penalty_slerp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_cat\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDtransformer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                     \u001b[0mpen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-27233b7eda73>\u001b[0m in \u001b[0;36mcalc_gradient_penalty_slerp\u001b[0;34m(netD, real_data, fake_data, transformer, device, lambda_)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0minterpolates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslerp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0minterpolates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0minterpolates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0minterpolates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mdisc_interpolates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4ef2ba3f06cd>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = ctabgan.generate_samples()\n",
        "sample_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ufZtAp6kwFe",
        "outputId": "d32eb652-364a-48c9-c581-08c5d40bb3b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48842, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_5btzFxzmGMs",
        "outputId": "9108ee85-3c10-4634-bed3-0c018edbd04e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age workclass  fnlwgt education marital-status occupation relationship  \\\n",
              "0   37         4  218421         8              4          7            3   \n",
              "1   43         7  381484        12              2          1            0   \n",
              "2   32         4  283416         9              4          4            3   \n",
              "3   38         1  214013        15              5          4            0   \n",
              "4   51         4  226985         9              4          3            1   \n",
              "\n",
              "  race gender  capital-gain  capital-loss  hours-per-week native-country  \\\n",
              "0    0      0             0             0              10             39   \n",
              "1    1      0          3859             0              40             39   \n",
              "2    4      1             0          1662              45             39   \n",
              "3    2      0          3330             0              40             39   \n",
              "4    1      1             0             0              32             39   \n",
              "\n",
              "  income  \n",
              "0      0  \n",
              "1      1  \n",
              "2      0  \n",
              "3      0  \n",
              "4      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0a29300-5484-4b68-963b-3a2709a7ab36\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>gender</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37</td>\n",
              "      <td>4</td>\n",
              "      <td>218421</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43</td>\n",
              "      <td>7</td>\n",
              "      <td>381484</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3859</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32</td>\n",
              "      <td>4</td>\n",
              "      <td>283416</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1662</td>\n",
              "      <td>45</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>214013</td>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3330</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>4</td>\n",
              "      <td>226985</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0a29300-5484-4b68-963b-3a2709a7ab36')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a0a29300-5484-4b68-963b-3a2709a7ab36 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a0a29300-5484-4b68-963b-3a2709a7ab36');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7e4ae626-b35e-4ea2-b451-d06ff901f1bf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e4ae626-b35e-4ea2-b451-d06ff901f1bf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7e4ae626-b35e-4ea2-b451-d06ff901f1bf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sample_df",
              "summary": "{\n  \"name\": \"sample_df\",\n  \"rows\": 48842,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 23,\n        \"max\": 62,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          26,\n          31,\n          42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"workclass\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"0\",\n          \"7\",\n          \"5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fnlwgt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95606,\n        \"min\": 12365,\n        \"max\": 799665,\n        \"num_unique_values\": 45220,\n        \"samples\": [\n          124692,\n          141069,\n          242726\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"education\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"8\",\n          \"12\",\n          \"6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"marital-status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"4\",\n          \"2\",\n          \"6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"occupation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"14\",\n          \"8\",\n          \"7\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relationship\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"3\",\n          \"0\",\n          \"4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"race\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1\",\n          \"3\",\n          \"4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1\",\n          \"0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"capital-gain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6331,\n        \"min\": 0,\n        \"max\": 99994,\n        \"num_unique_values\": 3886,\n        \"samples\": [\n          7729,\n          8513\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"capital-loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 562,\n        \"min\": 0,\n        \"max\": 3135,\n        \"num_unique_values\": 993,\n        \"samples\": [\n          1494,\n          2406\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hours-per-week\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 1,\n        \"max\": 95,\n        \"num_unique_values\": 90,\n        \"samples\": [\n          6,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"native-country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"0\",\n          \"15\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"income\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1\",\n          \"0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3X8oByUmI4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}